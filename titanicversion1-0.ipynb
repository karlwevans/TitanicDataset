{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n\n# Input data files are available in the read-only \"../input/\" directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:28:58.734139Z","iopub.execute_input":"2025-05-10T06:28:58.734570Z","iopub.status.idle":"2025-05-10T06:28:58.743788Z","shell.execute_reply.started":"2025-05-10T06:28:58.734542Z","shell.execute_reply":"2025-05-10T06:28:58.742106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nimport re\n# Load Data\ntest_data=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntrain_data=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data.info()\ntrain_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T12:19:15.278031Z","iopub.execute_input":"2025-05-14T12:19:15.278409Z","iopub.status.idle":"2025-05-14T12:19:18.444987Z","shell.execute_reply.started":"2025-05-14T12:19:15.278381Z","shell.execute_reply":"2025-05-14T12:19:18.443911Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 418 entries, 0 to 417\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  418 non-null    int64  \n 1   Pclass       418 non-null    int64  \n 2   Name         418 non-null    object \n 3   Sex          418 non-null    object \n 4   Age          332 non-null    float64\n 5   SibSp        418 non-null    int64  \n 6   Parch        418 non-null    int64  \n 7   Ticket       418 non-null    object \n 8   Fare         417 non-null    float64\n 9   Cabin        91 non-null     object \n 10  Embarked     418 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 36.0+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Process useless fields\n## Cabin  \n - extract deck  \n - some cabins are grouped\n - even/odd = port/starboard https://www.encyclopedia-titanica.org/titanic-deckplans/c-deck.html\n## Ticket  \n - extract prefix\n - some tickets cover many passengers     \n## Name  \n - extract title  \n - extract surname","metadata":{}},{"cell_type":"code","source":"X=train_data.dropna()\nsns.countplot(x='Port', hue='Survived',data=X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T04:59:34.471089Z","iopub.execute_input":"2025-05-11T04:59:34.471438Z","iopub.status.idle":"2025-05-11T04:59:34.773622Z","shell.execute_reply.started":"2025-05-11T04:59:34.471414Z","shell.execute_reply":"2025-05-11T04:59:34.772690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cabin\n############################################################################################################################################\n\"\"\"\n- Count of cabin groupings\n- Cabin grouping\n- Deck\n\"\"\"  \n\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Cabin_count'] = dataset['Cabin'].apply(lambda x: len(str(x).split()) if pd.notnull(x) else 0)\n    dataset['Deck'] = dataset['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n    \n   # dataset['Port'] =dataset['Cabin'].astype(str).str.extract(r'(\\d+)')\n   # dataset['Port']= pd.to_numeric(dataset['Port'], errors='coerce').astype('Int64')%2==0\n\n    idx = dataset[dataset['Deck'] == 'T'].index\n    dataset.loc[idx, 'Deck'] = 'A'\n    dataset['Deck'] = dataset['Deck'].replace(['A', 'B', 'C'], 'ABC')\n    dataset['Deck'] = dataset['Deck'].replace(['D', 'E'], 'DE')\n    dataset['Deck'] = dataset['Deck'].replace(['F', 'G'], 'FG')\n\n    dataset=dataset.drop([\"Cabin\"], axis=1)\n\"\"\"\nimport re\ndeck = {\"A\": \"A\", \"B\": \"B\", \"C\": \"C\", \"D\": \"D\", \"E\": \"E\", \"F\": \"F\", \"G\": \"G\", \"U\": \"U\"}\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    idx = dataset[dataset['Deck'] == 'T'].index\n    dataset.loc[idx, 'Deck'] = 'A'\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(\"U\")\n    #dataset['Deck'] = dataset['Deck'].astype(int)\n\"\"\"\n# we can now drop the cabin feature\ntrain_data = train_data.drop(['Cabin'], axis=1)\ntest_data = test_data.drop(['Cabin'], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T12:19:24.246251Z","iopub.execute_input":"2025-05-14T12:19:24.246772Z","iopub.status.idle":"2025-05-14T12:19:24.274817Z","shell.execute_reply.started":"2025-05-14T12:19:24.246734Z","shell.execute_reply":"2025-05-14T12:19:24.273716Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Ticket\n############################################################################################################################################\n\"\"\"\n- Prefix\n- Count \n- common ticket groupings\n\"\"\"\n\n# remove dots and slashes\ndef get_prefix(ticket):\n    lead = ticket.split(' ')[0][0]\n    if lead.isalpha():\n        return 'Prefixed' #ticket.split(' ')[0]\n    else:\n        return 'NoPrefix'\n\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Ticket'] = dataset['Ticket'].replace('LINE','line 0')\n    dataset['Ticket'] = dataset['Ticket'].apply(lambda x: x.replace('.','').replace('/','').lower())\n    dataset['Prefix'] = dataset['Ticket'].apply(lambda x: get_prefix(x))\n\n    dataset['TicketNumber'] = dataset['Ticket'].apply(lambda x: int(x.split(' ')[-1])//1)\n    dataset['TNlength'] = dataset['TicketNumber'].apply(lambda x : len(str(x)))\n    dataset['LeadingDigit'] = dataset['TicketNumber'].apply(lambda x : int(str(x)[0]))\n    # TGroup is numeric ticket with last digit removed - creates another grouping\n    dataset['TicketGroup'] = dataset['Ticket'].apply(lambda x: str(int(x.split(' ')[-1])//10))\n    dataset['TicketGroup'] = pd.to_numeric(dataset['TicketGroup'], downcast='integer', errors='coerce')\n\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Companions'] = dataset['Ticket'].duplicated(keep=False).astype(int) * dataset.groupby('Ticket')['Ticket'].transform('count') -1 \n    dataset.loc[dataset['Companions'] == -1, 'Companions'] = 0\n    # same as companions +1\n    dataset['Ticket_Frequency'] = dataset.groupby('Ticket')['Ticket'].transform('count')\n    # dup_tickets = train_data.groupby('Ticket').size()\n    # train_data['DupTickets'] = train_data['Ticket'].map(dup_tickets)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T12:19:28.553697Z","iopub.execute_input":"2025-05-14T12:19:28.554059Z","iopub.status.idle":"2025-05-14T12:19:28.590656Z","shell.execute_reply.started":"2025-05-14T12:19:28.554029Z","shell.execute_reply":"2025-05-14T12:19:28.589649Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Name\n############################################################################################################################################\n\"\"\"\n- Title\n- Surname groupings\n\"\"\"\n\ndef extract_surname(data):    \n    families = []\n    for i in range(len(data)):        \n        name = data.iloc[i]\n        if '(' in name:\n            name_no_bracket = name.split('(')[0] \n        else:\n            name_no_bracket = name\n        family = name_no_bracket.split(',')[0]\n        title = name_no_bracket.split(',')[1].strip().split(' ')[0]\n        \n        for c in string.punctuation:\n            family = family.replace(c, '').strip()\n            \n        families.append(family)           \n    return families\n    \ndata = [train_data, test_data]\nfor dataset in data:\n    dataset['Family'] = extract_surname(dataset['Name'])\n\ndef get_title(name):\n    title_search=re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nTitle_Dictionary={\"Capt\": \"Officer\", \"Col\":\"Officer\", \"Major\":\"Officer\",\n                  \"Jonkheer\":\"Royal\",\"Don\":\"Royal\",\"Sir\":\"Royal\", \"Lady\":\"Royal\",\n                  \"Dr\":\"Officer\",\"Rev\":\"Officer\",\n                  \"Countess\":\"Royal\", \"Dona\":\"Royal\",\n                  \"Mme\":\"Mrs\", \"Ms\":\"Mrs\", \"Mrs\":\"Mrs\",\n                  \"Mlle\":\"Miss\",   \"Miss\":\"Miss\",\n                  \"Mr\":\"Mr\",\n                  \"Master\":\"Master\"}\n\ndef titlemap(x):\n    return Title_Dictionary[x]\n    \ndata = [train_data, test_data]\nfor dataset in data:\n    dataset[\"Title\"]=dataset[\"Name\"].apply(get_title)\n    dataset[\"Title\"]=dataset[\"Title\"].apply(titlemap)\n   \ntrain_data=train_data.drop([\"Name\"], axis=1)\ntest_data=test_data.drop([\"Name\"], axis=1)\n\"\"\"\ndf_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T12:19:31.665892Z","iopub.execute_input":"2025-05-14T12:19:31.666224Z","iopub.status.idle":"2025-05-14T12:19:31.705374Z","shell.execute_reply.started":"2025-05-14T12:19:31.666197Z","shell.execute_reply":"2025-05-14T12:19:31.704394Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"\"\\ndf_all['Title'] = df_all['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\\ndf_all['Is_Married'] = 0\\ndf_all['Is_Married'].loc[df_all['Title'] == 'Mrs'] = 1\\n\""},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_data.Prefix.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:08:53.320645Z","iopub.execute_input":"2025-05-14T00:08:53.320983Z","iopub.status.idle":"2025-05-14T00:08:53.328618Z","shell.execute_reply.started":"2025-05-14T00:08:53.320954Z","shell.execute_reply":"2025-05-14T00:08:53.327650Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"Prefix\nNoPrefix    661\nPrefixed    230\nName: count, dtype: int64"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"train_data.info()\ntest_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:10:35.097668Z","iopub.execute_input":"2025-05-14T00:10:35.098070Z","iopub.status.idle":"2025-05-14T00:10:35.118097Z","shell.execute_reply.started":"2025-05-14T00:10:35.098036Z","shell.execute_reply":"2025-05-14T00:10:35.117033Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 22 columns):\n #   Column            Non-Null Count  Dtype   \n---  ------            --------------  -----   \n 0   PassengerId       891 non-null    int64   \n 1   Survived          891 non-null    int64   \n 2   Pclass            891 non-null    int64   \n 3   Sex               891 non-null    object  \n 4   Age               891 non-null    float64 \n 5   SibSp             891 non-null    int64   \n 6   Parch             891 non-null    int64   \n 7   Ticket            891 non-null    object  \n 8   Fare              891 non-null    float64 \n 9   Embarked          891 non-null    object  \n 10  Cabin_count       891 non-null    int64   \n 11  Deck              891 non-null    object  \n 12  Prefix            891 non-null    object  \n 13  TicketNumber      891 non-null    int64   \n 14  TNlength          891 non-null    int64   \n 15  LeadingDigit      891 non-null    int64   \n 16  TicketGroup       891 non-null    int32   \n 17  Companions        891 non-null    int64   \n 18  Ticket_Frequency  891 non-null    int64   \n 19  Family            891 non-null    object  \n 20  Title             891 non-null    object  \n 21  AgeBand           714 non-null    category\ndtypes: category(1), float64(2), int32(1), int64(11), object(7)\nmemory usage: 143.9+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 418 entries, 0 to 417\nData columns (total 20 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   PassengerId       418 non-null    int64  \n 1   Pclass            418 non-null    int64  \n 2   Sex               418 non-null    object \n 3   Age               418 non-null    float64\n 4   SibSp             418 non-null    int64  \n 5   Parch             418 non-null    int64  \n 6   Ticket            418 non-null    object \n 7   Fare              418 non-null    float64\n 8   Embarked          418 non-null    object \n 9   Cabin_count       418 non-null    int64  \n 10  Deck              418 non-null    object \n 11  Prefix            418 non-null    object \n 12  TicketNumber      418 non-null    int64  \n 13  TNlength          418 non-null    int64  \n 14  LeadingDigit      418 non-null    int64  \n 15  TicketGroup       418 non-null    int32  \n 16  Companions        418 non-null    int64  \n 17  Ticket_Frequency  418 non-null    int64  \n 18  Family            418 non-null    object \n 19  Title             418 non-null    object \ndtypes: float64(2), int32(1), int64(10), object(7)\nmemory usage: 63.8+ KB\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"# Missing Values\n - Age: subgroup medians, others have sampled uniform [20,40] or normal distr  \n - Embarked: researched true value\n - Fare: subgroup median  \n - Port: from shared ticket","metadata":{}},{"cell_type":"code","source":"# Filling the missing values in Age with the medians of Sex and Pclass groups\ndata = [train_data, test_data]\nfor dataset in data:\n    grouped = dataset.groupby(['Sex', 'Pclass'])['Age'].transform('median')\n    dataset['Age']=dataset['Age'].fillna(grouped)\n    #dataset['Age']=dataset['Age'].fillna(grouped)\n    dataset['Embarked'] = dataset['Embarked'].fillna('S') # commonly known datapoint\n\n    med_fare = dataset.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\n    dataset['Fare'] = dataset['Fare'].fillna(med_fare)\n\n\"\"\"\nfrom sklearn.impute import KNNImputer, SimpleImputer\n\nimputer=SimpleImputer(strategy='constant',#'mean',\n                      fill_value=-999,add_indicator=True).set_output(transform=\"pandas\")\n#imputer=KNNImputer(add_indicator=True).set_output(transform=\"pandas\")\n\ntrain_data=imputer.fit_transform(train_data)\ntest_data=imputer.fit_transform(test_data)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T12:19:41.071687Z","iopub.execute_input":"2025-05-14T12:19:41.072051Z","iopub.status.idle":"2025-05-14T12:19:41.097894Z","shell.execute_reply.started":"2025-05-14T12:19:41.072023Z","shell.execute_reply":"2025-05-14T12:19:41.096813Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'\\nfrom sklearn.impute import KNNImputer, SimpleImputer\\n\\nimputer=SimpleImputer(strategy=\\'constant\\',#\\'mean\\',\\n                      fill_value=-999,add_indicator=True).set_output(transform=\"pandas\")\\n#imputer=KNNImputer(add_indicator=True).set_output(transform=\"pandas\")\\n\\ntrain_data=imputer.fit_transform(train_data)\\ntest_data=imputer.fit_transform(test_data)\\n'"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Binning","metadata":{}},{"cell_type":"code","source":"data = [train_data, test_data]\nfor dataset in data:\n    dataset['AgeBand']=pd.cut(dataset['Age'], bins=[0,5,12,18,35,60,np.inf], labels=[0,1,2,3,4,5])\n    #dataset['Age'] = pd.qcut(dataset['Age'], 10)\n    dataset['AgeBand']=dataset['AgeBand'].astype(int)\n    dataset['FareBand']=pd.qcut(dataset['Fare'],q=13, labels=[0,1,2,3,4,5,6,7,8,9,10,11,12])\n    dataset['FareBand']=dataset['FareBand'].astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T12:19:44.375949Z","iopub.execute_input":"2025-05-14T12:19:44.376328Z","iopub.status.idle":"2025-05-14T12:19:44.398187Z","shell.execute_reply.started":"2025-05-14T12:19:44.376295Z","shell.execute_reply":"2025-05-14T12:19:44.397058Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Engineering","metadata":{}},{"cell_type":"code","source":"data = [train_data, test_data]\nfor dataset in data:\n    dataset['Family_Size']=dataset['SibSp']+dataset['Parch']+1\n    #dataset['ParchXPclass']=dataset.Parch*dataset.Pclass\n    #dataset['SibSpXPclass']=dataset.SibSp*dataset.Pclass\n    dataset['AgeXPclass']=dataset.Age*dataset.Pclass\n    dataset['AgeXFare']=dataset.Age*dataset.Fare\n    \n    Family_size=dataset.Parch+dataset.SibSp\n    dataset['Alone']=Family_size==0\n\n    family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\n    dataset['Family_Size_Grouped'] = dataset['Family_Size'].map(family_map)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T12:19:48.231217Z","iopub.execute_input":"2025-05-14T12:19:48.231589Z","iopub.status.idle":"2025-05-14T12:19:48.246565Z","shell.execute_reply.started":"2025-05-14T12:19:48.231555Z","shell.execute_reply":"2025-05-14T12:19:48.245221Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_data.info()\ntest_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T12:19:59.092200Z","iopub.execute_input":"2025-05-14T12:19:59.092633Z","iopub.status.idle":"2025-05-14T12:19:59.115885Z","shell.execute_reply.started":"2025-05-14T12:19:59.092598Z","shell.execute_reply":"2025-05-14T12:19:59.114713Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 28 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   PassengerId          891 non-null    int64  \n 1   Survived             891 non-null    int64  \n 2   Pclass               891 non-null    int64  \n 3   Sex                  891 non-null    object \n 4   Age                  891 non-null    float64\n 5   SibSp                891 non-null    int64  \n 6   Parch                891 non-null    int64  \n 7   Ticket               891 non-null    object \n 8   Fare                 891 non-null    float64\n 9   Embarked             891 non-null    object \n 10  Cabin_count          891 non-null    int64  \n 11  Deck                 891 non-null    object \n 12  Prefix               891 non-null    object \n 13  TicketNumber         891 non-null    int64  \n 14  TNlength             891 non-null    int64  \n 15  LeadingDigit         891 non-null    int64  \n 16  TicketGroup          891 non-null    int32  \n 17  Companions           891 non-null    int64  \n 18  Ticket_Frequency     891 non-null    int64  \n 19  Family               891 non-null    object \n 20  Title                891 non-null    object \n 21  AgeBand              891 non-null    int64  \n 22  FareBand             891 non-null    int64  \n 23  Family_Size          891 non-null    int64  \n 24  AgeXPclass           891 non-null    float64\n 25  AgeXFare             891 non-null    float64\n 26  Alone                891 non-null    bool   \n 27  Family_Size_Grouped  891 non-null    object \ndtypes: bool(1), float64(4), int32(1), int64(14), object(8)\nmemory usage: 185.5+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 418 entries, 0 to 417\nData columns (total 27 columns):\n #   Column               Non-Null Count  Dtype  \n---  ------               --------------  -----  \n 0   PassengerId          418 non-null    int64  \n 1   Pclass               418 non-null    int64  \n 2   Sex                  418 non-null    object \n 3   Age                  418 non-null    float64\n 4   SibSp                418 non-null    int64  \n 5   Parch                418 non-null    int64  \n 6   Ticket               418 non-null    object \n 7   Fare                 418 non-null    float64\n 8   Embarked             418 non-null    object \n 9   Cabin_count          418 non-null    int64  \n 10  Deck                 418 non-null    object \n 11  Prefix               418 non-null    object \n 12  TicketNumber         418 non-null    int64  \n 13  TNlength             418 non-null    int64  \n 14  LeadingDigit         418 non-null    int64  \n 15  TicketGroup          418 non-null    int32  \n 16  Companions           418 non-null    int64  \n 17  Ticket_Frequency     418 non-null    int64  \n 18  Family               418 non-null    object \n 19  Title                418 non-null    object \n 20  AgeBand              418 non-null    int64  \n 21  FareBand             418 non-null    int64  \n 22  Family_Size          418 non-null    int64  \n 23  AgeXPclass           418 non-null    float64\n 24  AgeXFare             418 non-null    float64\n 25  Alone                418 non-null    bool   \n 26  Family_Size_Grouped  418 non-null    object \ndtypes: bool(1), float64(4), int32(1), int64(13), object(8)\nmemory usage: 83.8+ KB\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Mutual Information","metadata":{}},{"cell_type":"code","source":"X = train_data.copy()\n#X=X.drop(['Cabin','Port'],axis=1)\ny = X.pop(\"Survived\")\nfor colname in X.select_dtypes(\"object\"):\n    X[colname], _ = X[colname].factorize()\n    \nX2 = test_data.copy()\n#X2=X2.drop(['Cabin','Port'],axis=1)\nfor colname in X2.select_dtypes(\"object\"):\n    X2[colname], _ = X2[colname].factorize()\n\n# All discrete features should now have integer dtypes (double-check this before using MI!)\ndiscrete_features = X.dtypes == int\ndiscrete_features\n\nfrom sklearn.feature_selection import mutual_info_classif\n\ndef make_mi_scores(X, y, discrete_features):\n    mi_scores = mutual_info_classif(X, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X, y, discrete_features)\nmi_scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T12:21:28.891170Z","iopub.execute_input":"2025-05-14T12:21:28.891540Z","iopub.status.idle":"2025-05-14T12:21:29.021918Z","shell.execute_reply.started":"2025-05-14T12:21:28.891502Z","shell.execute_reply":"2025-05-14T12:21:29.020809Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"PassengerId            0.665912\nTicket                 0.572496\nTicketNumber           0.570940\nFamily                 0.548683\nTitle                  0.169489\nSex                    0.150870\nFare                   0.137645\nTicketGroup            0.114302\nAgeXPclass             0.111545\nAgeXFare               0.082590\nFareBand               0.067594\nPclass                 0.058107\nCompanions             0.057587\nTicket_Frequency       0.057587\nLeadingDigit           0.053648\nDeck                   0.051222\nCabin_count            0.051054\nFamily_Size            0.047781\nAlone                  0.047016\nFamily_Size_Grouped    0.042141\nAge                    0.034382\nTNlength               0.026846\nSibSp                  0.023197\nParch                  0.016366\nEmbarked               0.014233\nAgeBand                0.012971\nPrefix                 0.000001\nName: MI Scores, dtype: float64"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# Pre Simple Fit","metadata":{}},{"cell_type":"code","source":"\"\"\"\ntrain_data2=pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf = train_data2.drop(['Ticket','Cabin'], axis=1)\n\ndf2=df[['Pclass','Sex','Age','SibSp','Parch','Embarked','Survived']]\ndf2['Pclass']=df2['Pclass'].astype(\"category\")\n\ndf2 = pd.concat([pd.get_dummies(df2[['Pclass','Sex','Embarked']], drop_first=True), \n                  df2[['Survived','Age','SibSp','Parch']]],axis=1)\n\ntest_data2=pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ngrouped = test_data2.groupby(['Sex', 'Pclass'])['Age'].transform('median')\ntest_data2['Age']=test_data2['Age'].fillna(grouped)\n\ndf3=test_data2[['Pclass','Sex','Age','SibSp','Parch','Embarked']]\ndf3['Pclass']=df3['Pclass'].astype(\"category\")\ndf3 = pd.concat([pd.get_dummies(df3[['Pclass','Sex','Embarked']], drop_first=True), \n                  df3[['Age','SibSp','Parch']]],axis=1)\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport sklearn.linear_model as lm\nimport sklearn.ensemble as en\nimport sklearn.gaussian_process as gp\nimport sklearn.svm as svm\nimport sklearn.discriminant_analysis as da \nimport sklearn.naive_bayes as nb\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost \n\nX_train, X_test,y_train, y_test=train_test_split(X,y, test_size=0.3, random_state=21)\n\n#instantiate and fit our model\n# Best = \n#results_rf = da.LinearDiscriminantAnalysis().fit(X_train, y_train) #2\nresults_rf = catboost.CatBoostClassifier(min_data_in_leaf=5,n_estimators=300,\n                                         verbose=0,max_depth= 10,l2_leaf_reg=5,\n                                        learning_rate=0.01).fit(X_train, y_train) #1\n#results_rf = lgb.LGBMClassifier(verbose=-1).fit(X_train, y_train) #3\n#results_rf = xgb.XGBClassifier().fit(X_train, y_train) #4\n#results_rf=lm.LogisticRegression().fit(X_train, y_train)\n# interesting......LDA is better than XGBoost?\n\n# The Rest\n#da.QuadraticDiscriminantAnalysis().fit(X_train, y_train)\n#gp.GaussianProcessClassifier().fit(X_train, y_train)\n#nb.GaussianNB().fit(X_train, y_train)\n#svm.SVC().fit(X_train, y_train)\n#en.HistGradientBoostingClassifier().fit(X_train, y_train)\n\n# Score the results\nscore = results_rf.score(X_test, y_test)\nprint(\"Accuracy of model on the data was: {0}\".format(score))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T12:36:20.266952Z","iopub.execute_input":"2025-05-14T12:36:20.267291Z","iopub.status.idle":"2025-05-14T12:36:24.245188Z","shell.execute_reply.started":"2025-05-14T12:36:20.267264Z","shell.execute_reply":"2025-05-14T12:36:24.243986Z"}},"outputs":[{"name":"stdout","text":"Accuracy of model on the data was: 0.8582089552238806\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['PassengerId']=test_data['PassengerId']\nsubmission['Survived'] = results_rf.predict(X2).tolist()\nsubmission.info()\nsubmission.to_csv(\"submission.csv\", index=False)\n# test score 0.7535","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T07:56:21.738409Z","iopub.execute_input":"2025-05-11T07:56:21.738750Z","iopub.status.idle":"2025-05-11T07:56:21.767293Z","shell.execute_reply.started":"2025-05-11T07:56:21.738724Z","shell.execute_reply":"2025-05-11T07:56:21.765254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis\nNumerical: 'Age', 'SibSp','Parch', 'Fare', 'TNlength', Cabin_count, Companions  \nOrdinal: Pclass, 'TicketNumber', 'LeadingDigit', 'TicketGroup',   \nCategorical: 'Sex', 'Family', 'Title','Embarked','Prefix', Deck  ","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T04:56:45.215745Z","iopub.execute_input":"2025-05-11T04:56:45.216153Z","iopub.status.idle":"2025-05-11T04:56:45.220330Z","shell.execute_reply.started":"2025-05-11T04:56:45.216126Z","shell.execute_reply":"2025-05-11T04:56:45.219167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#df_plot = train_data.groupby(['Survived','Family']).size().reset_index().pivot(columns='Survived', index='Family', values=0)\n#df_plot.plot(kind='bar', stacked=True)\ndef plot_stack(column_1, column_2):\n plot_stck=pd.crosstab(index=column_1, columns=column_2)\n plot_stck.plot(kind='barh', figsize=(8,8), stacked=True)\n return\nplot_stack(train_data['Family'],train_data['Survived'])\n#train_data.head()\n#sns.countplot(x='Family',hue='Survived',data=train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T07:43:59.008749Z","iopub.execute_input":"2025-05-10T07:43:59.009168Z","iopub.status.idle":"2025-05-10T07:44:05.940005Z","shell.execute_reply.started":"2025-05-10T07:43:59.009137Z","shell.execute_reply":"2025-05-10T07:44:05.938774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=2,sharex=True, figsize=(10, 8))\nsns.violinplot(y='Fare',x='LeadingDigit', hue='Survived',data=train_data, kde=True,                split=True, ax=axs[0])\nsns.violinplot(y='Age',x='LeadingDigit', hue='Survived',data=train_data, kde=True,                split=True, ax=axs[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T08:03:43.058525Z","iopub.execute_input":"2025-05-10T08:03:43.058994Z","iopub.status.idle":"2025-05-10T08:03:44.150068Z","shell.execute_reply.started":"2025-05-10T08:03:43.058961Z","shell.execute_reply":"2025-05-10T08:03:44.148459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.countplot(x='Cabin_count', hue='Survived', data=train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T07:49:57.760625Z","iopub.execute_input":"2025-05-10T07:49:57.761043Z","iopub.status.idle":"2025-05-10T07:49:57.983375Z","shell.execute_reply.started":"2025-05-10T07:49:57.761011Z","shell.execute_reply":"2025-05-10T07:49:57.982461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axs = plt.subplots(nrows=3,sharex=True, figsize=(10, 8))\nsns.countplot(x=\"Parch\",hue=\"Survived\", data=train_data, ax=axs[0]\n             #bins=40, element=\"step\", stat=\"density\", common_norm=False#kde=True, #multiple=\"stack\"\n           )\nsns.countplot(x=\"SibSp\",hue=\"Survived\", data=train_data, ax=axs[1])\nsns.countplot(x=\"Family_Size\",hue=\"Survived\",data=train_data, ax=axs[2])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T11:39:15.625414Z","iopub.execute_input":"2025-05-11T11:39:15.625810Z","iopub.status.idle":"2025-05-11T11:39:16.391327Z","shell.execute_reply.started":"2025-05-11T11:39:15.625784Z","shell.execute_reply":"2025-05-11T11:39:16.390254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.histplot(x='Family_Size_Grouped', hue='Survived',data=train_data, kde=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:37:02.241346Z","iopub.execute_input":"2025-05-11T12:37:02.241703Z","iopub.status.idle":"2025-05-11T12:37:02.527606Z","shell.execute_reply.started":"2025-05-11T12:37:02.241670Z","shell.execute_reply":"2025-05-11T12:37:02.526366Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig, axs = plt.subplots(ncols=5,nrows=4, figsize=(15, 20))\nsns.scatterplot(y='LeadingDigit', x='SibSp', hue='Survived', data=train_data, alpha=0.2, ax=axs[0,0])\nsns.scatterplot(y='LeadingDigit', x='Parch', hue='Survived', data=train_data, alpha=0.2, ax=axs[0,1])\nsns.scatterplot(y='LeadingDigit', x='TNlength', hue='Survived', data=train_data, alpha=0.2, ax=axs[0,2])\nsns.scatterplot(y='LeadingDigit', x='Cabin_count', hue='Survived', data=train_data, alpha=0.2, ax=axs[0,3])\nsns.scatterplot(y='LeadingDigit', x='TicketGroup', hue='Survived', data=train_data, alpha=0.2, ax=axs[0,4])\nsns.scatterplot(y='Pclass', x='SibSp', hue='Survived', data=train_data, alpha=0.2, ax=axs[1,0])\nsns.scatterplot(y='Pclass', x='Parch', hue='Survived', data=train_data, alpha=0.2, ax=axs[1,1])\nsns.scatterplot(y='Pclass', x='TNlength', hue='Survived', data=train_data, alpha=0.2, ax=axs[1,2])\nsns.scatterplot(y='Pclass', x='Cabin_count', hue='Survived', data=train_data, alpha=0.2, ax=axs[1,3])\nsns.scatterplot(y='Pclass', x='TicketGroup', hue='Survived', data=train_data, alpha=0.2, ax=axs[1,4])\nsns.scatterplot(y='TicketNumber', x='SibSp', hue='Survived', data=train_data, alpha=0.2, ax=axs[2,0])\nsns.scatterplot(y='TicketNumber', x='Parch', hue='Survived', data=train_data, alpha=0.2, ax=axs[2,1])\nsns.scatterplot(y='TicketNumber', x='TNlength', hue='Survived', data=train_data, alpha=0.2, ax=axs[2,2])\nsns.scatterplot(y='TicketNumber', x='Cabin_count', hue='Survived', data=train_data, alpha=0.2, ax=axs[2,3])\nsns.scatterplot(y='TicketNumber', x='TicketGroup', hue='Survived', data=train_data, alpha=0.2, ax=axs[2,4])\nsns.scatterplot(y='TicketGroup', x='SibSp', hue='Survived', data=train_data, alpha=0.2, ax=axs[3,0])\nsns.scatterplot(y='TicketGroup', x='Parch', hue='Survived', data=train_data, alpha=0.2, ax=axs[3,1])\nsns.scatterplot(y='TicketGroup', x='TNlength', hue='Survived', data=train_data, alpha=0.2, ax=axs[3,2])\nsns.scatterplot(y='TicketGroup', x='Cabin_count', hue='Survived', data=train_data, alpha=0.2, ax=axs[3,3])\nsns.scatterplot(y='TicketGroup', x='TicketGroup', hue='Survived', data=train_data, alpha=0.2, ax=axs[3,4])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T08:22:50.713052Z","iopub.execute_input":"2025-05-10T08:22:50.713443Z","iopub.status.idle":"2025-05-10T08:22:56.659187Z","shell.execute_reply.started":"2025-05-10T08:22:50.713413Z","shell.execute_reply":"2025-05-10T08:22:56.657926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.relplot(x=\"Age\",y='Fare',col='Sex', row='Deck',hue=\"Survived\",data=train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T08:24:25.480612Z","iopub.execute_input":"2025-05-10T08:24:25.481009Z","iopub.status.idle":"2025-05-10T08:24:29.194185Z","shell.execute_reply.started":"2025-05-10T08:24:25.480980Z","shell.execute_reply":"2025-05-10T08:24:29.192977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.violinplot(x =\"Sex\", y =\"Age\", hue =\"Survived\",  data = train_data, split = True) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T23:40:23.791440Z","iopub.execute_input":"2025-05-08T23:40:23.791766Z","iopub.status.idle":"2025-05-08T23:40:24.060890Z","shell.execute_reply.started":"2025-05-08T23:40:23.791741Z","shell.execute_reply":"2025-05-08T23:40:24.059968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\n# Experimenting with Bin Number\ntrain_data['Fare_Range'] = pd.qcut(train_data['Fare'], 30) \ntrain_data['Age_Range'] = pd.qcut(train_data['Age'], 12) \ncols=['Fare_Range','Age_Range']\nX = train_data[cols]\nfor colname in X.select_dtypes(\"category\"):\n    X[colname], _ = X[colname].factorize()\ny = train_data[\"Survived\"]\ndiscrete_features = X.dtypes == int\n\nmi_scores = mutual_info_classif(X, y,discrete_features=discrete_features)\n\ntrain_data=train_data.drop('Fare_Range', axis=1)\ntrain_data=train_data.drop('Age_Range', axis=1)\nmi_scores\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T05:07:17.603966Z","iopub.execute_input":"2025-05-11T05:07:17.604294Z","iopub.status.idle":"2025-05-11T05:07:17.641853Z","shell.execute_reply.started":"2025-05-11T05:07:17.604271Z","shell.execute_reply":"2025-05-11T05:07:17.640897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.relplot(x=\"Age\",y='Fare',col='Embarked', row='Pclass',hue=\"Survived\",data=train_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.histplot(x=\"Fare\",y='Parch',hue=\"Survived\", bins=50,data=train_data)#.set_xscale('log')\nsns.histplot(x=\"SibSp\",y='Parch',hue=\"Survived\",data=train_data).set_yscale('log')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T01:43:44.253038Z","iopub.execute_input":"2025-05-05T01:43:44.253423Z","iopub.status.idle":"2025-05-05T01:43:44.763841Z","shell.execute_reply.started":"2025-05-05T01:43:44.253393Z","shell.execute_reply":"2025-05-05T01:43:44.762382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.histplot(x=\"Parch\",hue=\"Survived\",bins =5,data=train_data).set_yscale('log')\nsns.histplot(x=\"Pclass\",hue=\"Survived\",bins=3,data=train_data)#.set_xscale('log')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T07:54:16.286749Z","iopub.execute_input":"2025-05-04T07:54:16.287105Z","iopub.status.idle":"2025-05-04T07:54:16.617589Z","shell.execute_reply.started":"2025-05-04T07:54:16.287076Z","shell.execute_reply":"2025-05-04T07:54:16.616832Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Categorical\nname, sex, embarked, cabin","metadata":{}},{"cell_type":"code","source":"sns.countplot(x=\"Sex\",hue=\"Survived\", data=train_data)\nsns.countplot(x=\"Embarked\",hue=\"Survived\", data=train_data)\nsns.countplot(x='Companions', hue='Survived', data=train_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T08:03:03.805418Z","iopub.execute_input":"2025-05-11T08:03:03.805761Z","iopub.status.idle":"2025-05-11T08:03:04.122873Z","shell.execute_reply.started":"2025-05-11T08:03:03.805736Z","shell.execute_reply":"2025-05-11T08:03:04.121424Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Processing  \n## Ideas  \nhttps://medium.com/@praoiticica/titanic-data-cleaning-and-feature-engineering-9f122752097f  \nhttps://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial/notebook   \n\n## Creating Features\n - Math transforms  \n - Counts (across cols): X[\"Components\"] = X[components].gt(0).sum(axis=1)  \n - Building up / breaking down  \n - Grouped  \n - clustering  \n - target encoding  \n - PCA  \n - ","metadata":{}},{"cell_type":"code","source":"\"\"\"\ndef concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)\n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ndf_all = concat_df(df_train, df_test)\n\ndf_train.name = 'Training Set'\ndf_test.name = 'Test Set'\ndf_all.name = 'All Set' \n\ndfs = [df_train, df_test]\n\nprint('Number of Training Examples = {}'.format(df_train.shape[0]))\nprint('Number of Test Examples = {}\\n'.format(df_test.shape[0]))\nprint('Training X Shape = {}'.format(df_train.shape))\nprint('Training y Shape = {}\\n'.format(df_train['Survived'].shape[0]))\nprint('Test X Shape = {}'.format(df_test.shape))\nprint('Test y Shape = {}\\n'.format(df_test.shape[0]))\nprint(df_train.columns)\nprint(df_test.columns)\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Engineering","metadata":{}},{"cell_type":"code","source":"\"\"\"\ndef isRare(title):\n    if title == \"Mr\" or title == \"Mrs\" or title == \"Master\" or title == \"Miss\":\n        return 0\n    return 1\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset[\"TitleRarity\"] = dataset[\"Title\"].apply(isRare)\n\nsns.countplot(x='TitleRarity', hue='Survived', data=train_data)\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Target Encoding","metadata":{}},{"cell_type":"code","source":"# Creating a list of families and tickets that are occuring in both training and test set\nnon_unique_families = [x for x in train_data['Family'].unique() if x in test_data['Family'].unique()]\nnon_unique_tickets = [x for x in train_data['Ticket'].unique() if x in test_data['Ticket'].unique()]\n\ndf_family_survival_rate = train_data.groupby('Family')[['Survived', 'Family_Size']].median()\ndf_ticket_survival_rate = train_data.groupby('Ticket')[['Survived', 'Ticket_Frequency']].median()\n\nfamily_rates = {}\nticket_rates = {}\n\nfor i in range(len(df_family_survival_rate)):\n    # Checking a family exists in both training and test set, and has members more than 1\n    if df_family_survival_rate.index[i] in non_unique_families and df_family_survival_rate.iloc[i, 1] > 1:\n        family_rates[df_family_survival_rate.index[i]] = df_family_survival_rate.iloc[i, 0]\n\nfor i in range(len(df_ticket_survival_rate)):\n    # Checking a ticket exists in both training and test set, and has members more than 1\n    if df_ticket_survival_rate.index[i] in non_unique_tickets and df_ticket_survival_rate.iloc[i, 1] > 1:\n        ticket_rates[df_ticket_survival_rate.index[i]] = df_ticket_survival_rate.iloc[i, 0]\n\nprint (df_family_survival_rate.head(2))\nprint (\"\\n\", list(family_rates.items())[:2])      \n\nprint (\"\\n\\n\", df_ticket_survival_rate.head(2))\nprint (\"\\n\", list(ticket_rates.items())[:2])       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:11:28.230190Z","iopub.execute_input":"2025-05-14T00:11:28.230548Z","iopub.status.idle":"2025-05-14T00:11:28.369396Z","shell.execute_reply.started":"2025-05-14T00:11:28.230518Z","shell.execute_reply":"2025-05-14T00:11:28.368619Z"}},"outputs":[{"name":"stdout","text":"        Survived  Family_Size\nFamily                       \nAbbing       0.0          1.0\nAbbott       0.5          3.0\n\n [('Abbott', 0.5), ('Aks', 1.0)]\n\n\n         Survived  Ticket_Frequency\nTicket                            \n110152       1.0               3.0\n110413       1.0               3.0\n\n [('113781', 0.5), ('11767', 1.0)]\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"mean_survival_rate = np.mean(train_data['Survived'])\n\ntrain_family_survival_rate = []\ntrain_family_survival_rate_NA = []\ntest_family_survival_rate = []\ntest_family_survival_rate_NA = []\n\nfor i in range(len(train_data)):\n    if train_data['Family'][i] in family_rates:\n        train_family_survival_rate.append(family_rates[train_data['Family'][i]])\n        train_family_survival_rate_NA.append(1)\n    else:\n        train_family_survival_rate.append(mean_survival_rate)\n        train_family_survival_rate_NA.append(0)\n        \nfor i in range(len(test_data)):\n    if test_data['Family'].iloc[i] in family_rates:\n        test_family_survival_rate.append(family_rates[test_data['Family'].iloc[i]])\n        test_family_survival_rate_NA.append(1)\n    else:\n        test_family_survival_rate.append(mean_survival_rate)\n        test_family_survival_rate_NA.append(0)\n        \ntrain_data['Family_Survival_Rate'] = train_family_survival_rate\ntrain_data['Family_Survival_Rate_NA'] = train_family_survival_rate_NA\ntest_data['Family_Survival_Rate'] = test_family_survival_rate\ntest_data['Family_Survival_Rate_NA'] = test_family_survival_rate_NA\n\ntrain_ticket_survival_rate = []\ntrain_ticket_survival_rate_NA = []\ntest_ticket_survival_rate = []\ntest_ticket_survival_rate_NA = []\n\nfor i in range(len(train_data)):\n    if train_data['Ticket'][i] in ticket_rates:\n        train_ticket_survival_rate.append(ticket_rates[train_data['Ticket'][i]])\n        train_ticket_survival_rate_NA.append(1)\n    else:\n        train_ticket_survival_rate.append(mean_survival_rate)\n        train_ticket_survival_rate_NA.append(0)\n        \nfor i in range(len(test_data)):\n    if test_data['Ticket'].iloc[i] in ticket_rates:\n        test_ticket_survival_rate.append(ticket_rates[test_data['Ticket'].iloc[i]])\n        test_ticket_survival_rate_NA.append(1)\n    else:\n        test_ticket_survival_rate.append(mean_survival_rate)\n        test_ticket_survival_rate_NA.append(0)\n        \ntrain_data['Ticket_Survival_Rate'] = train_ticket_survival_rate\ntrain_data['Ticket_Survival_Rate_NA'] = train_ticket_survival_rate_NA\ntest_data['Ticket_Survival_Rate'] = test_ticket_survival_rate\ntest_data['Ticket_Survival_Rate_NA'] = test_ticket_survival_rate_NA\n\n#\nfor df in [train_data, test_data]:\n    df['Survival_Rate'] = (df['Ticket_Survival_Rate'] + df['Family_Survival_Rate']) / 2\n    df['Survival_Rate_NA'] = (df['Ticket_Survival_Rate_NA'] + df['Family_Survival_Rate_NA']) / 2 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:11:31.169622Z","iopub.execute_input":"2025-05-14T00:11:31.170014Z","iopub.status.idle":"2025-05-14T00:11:31.215727Z","shell.execute_reply.started":"2025-05-14T00:11:31.169980Z","shell.execute_reply":"2025-05-14T00:11:31.214416Z"}},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":"# Transformation","metadata":{}},{"cell_type":"code","source":"train_data.head().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:42:22.753861Z","iopub.execute_input":"2025-05-11T12:42:22.754242Z","iopub.status.idle":"2025-05-11T12:42:22.770540Z","shell.execute_reply.started":"2025-05-11T12:42:22.754215Z","shell.execute_reply":"2025-05-11T12:42:22.769475Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n# Label and One Hot Encoding\nnon_numeric_features = ['Embarked', 'Sex', 'Deck', 'Title', 'Family_Size_Grouped', 'AgeBand', 'FareBand','Prefix']\n\nfor df in [train_data, test_data]:\n    for feature in non_numeric_features:        \n        df[feature] = LabelEncoder().fit_transform(df[feature])\n\ncat_features = ['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped','Prefix']\n\nprint (f\"Before one hot encoding cat features: {train_data.shape}\") \ntrain_data = pd.get_dummies(data=train_data, columns=cat_features, drop_first=True)\nprint (f\"After one hot encoding cat features: {train_data.shape}\") \n\nprint (f\"Before one hot encoding cat features: {test_data.shape}\") \ntest_data = pd.get_dummies(data=test_data, columns=cat_features, drop_first=True)\nprint (f\"After one hot encoding cat features: {test_data.shape}\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:11:35.143702Z","iopub.execute_input":"2025-05-14T00:11:35.144085Z","iopub.status.idle":"2025-05-14T00:11:35.176107Z","shell.execute_reply.started":"2025-05-14T00:11:35.144054Z","shell.execute_reply":"2025-05-14T00:11:35.175077Z"}},"outputs":[{"name":"stdout","text":"Before one hot encoding cat features: (891, 34)\nAfter one hot encoding cat features: (891, 44)\nBefore one hot encoding cat features: (418, 33)\nAfter one hot encoding cat features: (418, 43)\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"train_data.info()\ntest_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:02:35.182972Z","iopub.execute_input":"2025-05-14T00:02:35.183315Z","iopub.status.idle":"2025-05-14T00:02:35.209942Z","shell.execute_reply.started":"2025-05-14T00:02:35.183285Z","shell.execute_reply":"2025-05-14T00:02:35.208766Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 72 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   PassengerId              891 non-null    int64  \n 1   Survived                 891 non-null    int64  \n 2   Age                      891 non-null    float64\n 3   SibSp                    891 non-null    int64  \n 4   Parch                    891 non-null    int64  \n 5   Ticket                   891 non-null    object \n 6   Fare                     891 non-null    float64\n 7   Cabin_count              891 non-null    int64  \n 8   TicketNumber             891 non-null    int64  \n 9   TNlength                 891 non-null    int64  \n 10  LeadingDigit             891 non-null    int64  \n 11  TicketGroup              891 non-null    int32  \n 12  Companions               891 non-null    int64  \n 13  Ticket_Frequency         891 non-null    int64  \n 14  AgeBand                  891 non-null    int64  \n 15  FareBand                 891 non-null    int64  \n 16  Family_Size              891 non-null    int64  \n 17  AgeXPclass               891 non-null    float64\n 18  AgeXFare                 891 non-null    float64\n 19  Alone                    891 non-null    bool   \n 20  Family                   891 non-null    object \n 21  Family_Survival_Rate     891 non-null    float64\n 22  Family_Survival_Rate_NA  891 non-null    int64  \n 23  Ticket_Survival_Rate     891 non-null    float64\n 24  Ticket_Survival_Rate_NA  891 non-null    int64  \n 25  Survival_Rate            891 non-null    float64\n 26  Survival_Rate_NA         891 non-null    float64\n 27  Pclass_2                 891 non-null    bool   \n 28  Pclass_3                 891 non-null    bool   \n 29  Sex_1                    891 non-null    bool   \n 30  Deck_1                   891 non-null    bool   \n 31  Deck_2                   891 non-null    bool   \n 32  Deck_3                   891 non-null    bool   \n 33  Embarked_1               891 non-null    bool   \n 34  Embarked_2               891 non-null    bool   \n 35  Title_1                  891 non-null    bool   \n 36  Title_2                  891 non-null    bool   \n 37  Title_3                  891 non-null    bool   \n 38  Title_4                  891 non-null    bool   \n 39  Title_5                  891 non-null    bool   \n 40  Family_Size_Grouped_1    891 non-null    bool   \n 41  Family_Size_Grouped_2    891 non-null    bool   \n 42  Family_Size_Grouped_3    891 non-null    bool   \n 43  Prefix_1                 891 non-null    bool   \n 44  Prefix_2                 891 non-null    bool   \n 45  Prefix_3                 891 non-null    bool   \n 46  Prefix_4                 891 non-null    bool   \n 47  Prefix_5                 891 non-null    bool   \n 48  Prefix_6                 891 non-null    bool   \n 49  Prefix_7                 891 non-null    bool   \n 50  Prefix_8                 891 non-null    bool   \n 51  Prefix_9                 891 non-null    bool   \n 52  Prefix_10                891 non-null    bool   \n 53  Prefix_11                891 non-null    bool   \n 54  Prefix_12                891 non-null    bool   \n 55  Prefix_13                891 non-null    bool   \n 56  Prefix_14                891 non-null    bool   \n 57  Prefix_15                891 non-null    bool   \n 58  Prefix_16                891 non-null    bool   \n 59  Prefix_17                891 non-null    bool   \n 60  Prefix_18                891 non-null    bool   \n 61  Prefix_19                891 non-null    bool   \n 62  Prefix_20                891 non-null    bool   \n 63  Prefix_21                891 non-null    bool   \n 64  Prefix_22                891 non-null    bool   \n 65  Prefix_23                891 non-null    bool   \n 66  Prefix_24                891 non-null    bool   \n 67  Prefix_25                891 non-null    bool   \n 68  Prefix_26                891 non-null    bool   \n 69  Prefix_27                891 non-null    bool   \n 70  Prefix_28                891 non-null    bool   \n 71  Prefix_29                891 non-null    bool   \ndtypes: bool(46), float64(8), int32(1), int64(15), object(2)\nmemory usage: 217.7+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 418 entries, 0 to 417\nData columns (total 68 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   PassengerId              418 non-null    int64  \n 1   Age                      418 non-null    float64\n 2   SibSp                    418 non-null    int64  \n 3   Parch                    418 non-null    int64  \n 4   Ticket                   418 non-null    object \n 5   Fare                     418 non-null    float64\n 6   Cabin_count              418 non-null    int64  \n 7   TicketNumber             418 non-null    int64  \n 8   TNlength                 418 non-null    int64  \n 9   LeadingDigit             418 non-null    int64  \n 10  TicketGroup              418 non-null    int32  \n 11  Companions               418 non-null    int64  \n 12  Ticket_Frequency         418 non-null    int64  \n 13  AgeBand                  418 non-null    int64  \n 14  FareBand                 418 non-null    int64  \n 15  Family_Size              418 non-null    int64  \n 16  AgeXPclass               418 non-null    float64\n 17  AgeXFare                 418 non-null    float64\n 18  Alone                    418 non-null    bool   \n 19  Family                   418 non-null    object \n 20  Family_Survival_Rate     418 non-null    float64\n 21  Family_Survival_Rate_NA  418 non-null    int64  \n 22  Ticket_Survival_Rate     418 non-null    float64\n 23  Ticket_Survival_Rate_NA  418 non-null    int64  \n 24  Survival_Rate            418 non-null    float64\n 25  Survival_Rate_NA         418 non-null    float64\n 26  Pclass_2                 418 non-null    bool   \n 27  Pclass_3                 418 non-null    bool   \n 28  Sex_1                    418 non-null    bool   \n 29  Deck_1                   418 non-null    bool   \n 30  Deck_2                   418 non-null    bool   \n 31  Deck_3                   418 non-null    bool   \n 32  Embarked_1               418 non-null    bool   \n 33  Embarked_2               418 non-null    bool   \n 34  Title_1                  418 non-null    bool   \n 35  Title_2                  418 non-null    bool   \n 36  Title_3                  418 non-null    bool   \n 37  Title_4                  418 non-null    bool   \n 38  Title_5                  418 non-null    bool   \n 39  Family_Size_Grouped_1    418 non-null    bool   \n 40  Family_Size_Grouped_2    418 non-null    bool   \n 41  Family_Size_Grouped_3    418 non-null    bool   \n 42  Prefix_1                 418 non-null    bool   \n 43  Prefix_2                 418 non-null    bool   \n 44  Prefix_3                 418 non-null    bool   \n 45  Prefix_4                 418 non-null    bool   \n 46  Prefix_5                 418 non-null    bool   \n 47  Prefix_6                 418 non-null    bool   \n 48  Prefix_7                 418 non-null    bool   \n 49  Prefix_8                 418 non-null    bool   \n 50  Prefix_9                 418 non-null    bool   \n 51  Prefix_10                418 non-null    bool   \n 52  Prefix_11                418 non-null    bool   \n 53  Prefix_12                418 non-null    bool   \n 54  Prefix_13                418 non-null    bool   \n 55  Prefix_14                418 non-null    bool   \n 56  Prefix_15                418 non-null    bool   \n 57  Prefix_16                418 non-null    bool   \n 58  Prefix_17                418 non-null    bool   \n 59  Prefix_18                418 non-null    bool   \n 60  Prefix_19                418 non-null    bool   \n 61  Prefix_20                418 non-null    bool   \n 62  Prefix_21                418 non-null    bool   \n 63  Prefix_22                418 non-null    bool   \n 64  Prefix_23                418 non-null    bool   \n 65  Prefix_24                418 non-null    bool   \n 66  Prefix_25                418 non-null    bool   \n 67  Prefix_26                418 non-null    bool   \ndtypes: bool(43), float64(8), int32(1), int64(14), object(2)\nmemory usage: 97.7+ KB\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"drop_cols = (\n    # 1. cat features (no need to need dropped as dummy_encoding takes care of them)\n    #['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\n    # 2. objects\n    [#'Name',\n        'Ticket', 'Family'] \n    # 3. other cols\n    + ['Family_Size', 'Parch', 'PassengerId', 'SibSp', 'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA'] \n    # 4. target col\n    + ['Survived'])\ndrop_cols2 = (\n    # 1. cat features (no need to need dropped as dummy_encoding takes care of them)\n    #['Pclass', 'Sex', 'Deck', 'Embarked', 'Title', 'Family_Size_Grouped']\n    # 2. objects\n    [#'Name',\n        'Ticket', 'Family'] \n    # 3. other cols\n    + ['Family_Size', 'Parch', 'PassengerId', 'SibSp', 'Ticket_Survival_Rate', 'Family_Survival_Rate', 'Ticket_Survival_Rate_NA', 'Family_Survival_Rate_NA'] \n    # 4. target col\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:11:45.509060Z","iopub.execute_input":"2025-05-14T00:11:45.509401Z","iopub.status.idle":"2025-05-14T00:11:45.514339Z","shell.execute_reply.started":"2025-05-14T00:11:45.509372Z","shell.execute_reply":"2025-05-14T00:11:45.513326Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nX = StandardScaler().fit_transform(train_data.drop(columns=drop_cols))\ny = train_data['Survived'].values\nX2 = StandardScaler().fit_transform(test_data.drop(columns=drop_cols2))\n\"\"\"\n# Manual Scaling\ndata = [train_data, test_data]\nfor dataset in data:\n    numeric_columns=['Age','SibSp','Parch','Fare','DupTickets','TNlen','LeadingDigit']\n    mu=dataset[numeric_columns].mean(axis=0)\n    std=dataset[numeric_columns].std(axis=0)\n    dataset[numeric_columns]=(dataset[numeric_columns]-mu/std)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:11:52.524344Z","iopub.execute_input":"2025-05-14T00:11:52.524716Z","iopub.status.idle":"2025-05-14T00:11:52.559260Z","shell.execute_reply.started":"2025-05-14T00:11:52.524684Z","shell.execute_reply":"2025-05-14T00:11:52.558328Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"\"\\n# Manual Scaling\\ndata = [train_data, test_data]\\nfor dataset in data:\\n    numeric_columns=['Age','SibSp','Parch','Fare','DupTickets','TNlen','LeadingDigit']\\n    mu=dataset[numeric_columns].mean(axis=0)\\n    std=dataset[numeric_columns].std(axis=0)\\n    dataset[numeric_columns]=(dataset[numeric_columns]-mu/std)\\n\""},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"y_train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:17:09.825584Z","iopub.execute_input":"2025-05-14T00:17:09.825963Z","iopub.status.idle":"2025-05-14T00:17:09.833266Z","shell.execute_reply.started":"2025-05-14T00:17:09.825901Z","shell.execute_reply":"2025-05-14T00:17:09.832277Z"}},"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"array([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n       1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n       1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n       1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n       1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n       1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0,\n       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n       0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n       1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1,\n       0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n       1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n       0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n       0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n       1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 1, 0, 0, 0, 1])"},"metadata":{}}],"execution_count":72},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"#feature selection\nimport catboost\nfrom sklearn.feature_selection import RFECV\nestimator=catboost.CatBoostClassifier(verbose=0)\nselector = RFECV(estimator, step=1, cv=5)\nselector = selector.fit(X_train, y_train)\nselector.ranking_","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T12:52:37.715514Z","iopub.execute_input":"2025-05-11T12:52:37.715906Z","iopub.status.idle":"2025-05-11T13:01:36.976564Z","shell.execute_reply.started":"2025-05-11T12:52:37.715872Z","shell.execute_reply":"2025-05-11T13:01:36.975645Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport sklearn.linear_model as lm\nimport sklearn.ensemble as en\nimport sklearn.gaussian_process as gp\nimport sklearn.svm as svm\nimport sklearn.discriminant_analysis as da \nimport sklearn.naive_bayes as nb\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost \nimport sklearn\n\nX_train, X_test,y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=21)\n\nmodel=lm.LogisticRegression()\n#catboost.CatBoostClassifier(verbose=0)\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\naccuracy =accuracy_score(y_test, y_pred)\nprint(f'Log Reg Accuracy:{accuracy:.4f}')\n\nmodel=catboost.CatBoostClassifier(verbose=0) #depth=6, iterations=500, learning_rate=0.01,l2_leaf_reg=5, \n   # 'depth':[3,1,2,6,4,5,7,8,9,10],\n    #          'iterations':[250,100,500,1000],\n    #          'learning_rate':[0.03,0.001,0.01,0.1,0.2,0.3],\n     #         'l2_leaf_reg':[3,1,5,10,100],\n      #        'border_count':[32,5,10,20,50,100,200],\n       #       'bagging_temperature':[0.03,0.09,0.25,0.75],\n        #      'random_strength':[0.2,0.5,0.8],\n         #'max_ctr_complexity':[1,2,3,4,5] \nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\naccuracy =accuracy_score(y_test, y_pred)\nprint(f'catboost Accuracy:{accuracy:.4f}')\n\nmodel=xgb.XGBClassifier()\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\naccuracy =accuracy_score(y_test, y_pred)\nprint(f'XGB Accuracy:{accuracy:.4f}')\n\nmodel=svm.SVC()\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\naccuracy =accuracy_score(y_test, y_pred)\nprint(f'SVM Accuracy:{accuracy:.4f}')\n\nmodel=da.LinearDiscriminantAnalysis()\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\naccuracy =accuracy_score(y_test, y_pred)\nprint(f'LDA Accuracy:{accuracy:.4f}')\n\nmodel=nb.GaussianNB()\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\naccuracy =accuracy_score(y_test, y_pred)\nprint(f'NB Accuracy:{accuracy:.4f}')\n\nmodel=lgb.LGBMClassifier(verbose=-1)\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\naccuracy =accuracy_score(y_test, y_pred)\nprint(f'LGBM Accuracy:{accuracy:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:08:15.463799Z","iopub.execute_input":"2025-05-14T01:08:15.464185Z"}},"outputs":[{"name":"stdout","text":"Log Reg Accuracy:0.8436\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# Submission\nsubmission = pd.DataFrame()\nsubmission['PassengerId']=pd.to_numeric(test_data['PassengerId'],downcast='integer')\nsubmission['Survived'] = pd.to_numeric(lgb_classifier.predict(X_test).tolist(),downcast='integer')\nsubmission.head()\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:35:13.468612Z","iopub.execute_input":"2025-05-04T14:35:13.468968Z","iopub.status.idle":"2025-05-04T14:35:13.508613Z","shell.execute_reply.started":"2025-05-04T14:35:13.468942Z","shell.execute_reply":"2025-05-04T14:35:13.507343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn import linear_model, svm\nfrom sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression\n#model=linear_model.LogisticRegression(penalty='elasticnet', dual= False, solver='saga', tol=0.001, C=100, max_iter=10000, l1_ratio=0.4)\n\n# LGBM\nmodel=catboost.CatBoostClassifier(verbose=0)\n\n#=lgb.LGBMClassifier(objective='binary', max_depth=6, learning_rate=0.007, n_estimators=100, verbose=-1,min_data_in_leaf=25, num_iterations=1000, num_leaves=100, max_bin=10)\n\n# SVM\n#model=svm.SVC(C=100, kernel='poly', degree=5, gamma='scale', coef0=0.0, shrinking=True, tol=0.0001, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovo', \n#break_ties=False)\n# LDA\n#model=sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n# QDA\n#model=sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis()\n# Gaussian Process\n#model=\n# Naive Bayes\n#model=sklearn.naive_bayes.GaussianNB()\n# Voting Classifier\n#model=sklearn.ensemble.VotingClassifier(\n# AdaBoost\n#model =sklearn.ensemble.AdaBoostClassifier(\n# Supervised Nearest Neighbors\n#model=sklearn.neighbors.KNeighborsClassifier() \n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))\n\n#log_reg.fit(X_train, y_train)\n#y_pred=log_reg.predict(X_test)\n\n#accuracy =accuracy_score(y_test, y_pred)\n#print(f'Accuracy:{accuracy:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:34:18.073175Z","iopub.execute_input":"2025-05-11T13:34:18.073589Z","iopub.status.idle":"2025-05-11T13:35:12.661716Z","shell.execute_reply.started":"2025-05-11T13:34:18.073558Z","shell.execute_reply":"2025-05-11T13:35:12.659941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calibration\nfrom sklearn.calibration import CalibratedClassifierCV\nX_train, X_calib, y_train, y_calib = train_test_split(X, y, random_state=42)\nbase_clf = linear_model.LogisticRegression()\nbase_clf.fit(X_train, y_train)\n\nfrom sklearn.frozen import FrozenEstimator\ncalibrated_clf = CalibratedClassifierCV(FrozenEstimator(base_clf))\ncalibrated_clf.fit(X_calib, y_calib)\n\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(calibrated_clf, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Simple Ensemble Model","metadata":{}},{"cell_type":"code","source":"from sklearn import svm\n\nX_train, X_test,y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=21)\n\nlgbm = lgb.LGBMClassifier(objective = 'binary', n_estimators=500, verbose=-1).fit(X_train, y_train)\nsvmc = svm.SVC(C = 5, probability = True).fit(X_train, y_train)\nlr=lm.LogisticRegression(penalty='elasticnet', dual= False, solver='saga', tol=0.001, C=100, max_iter=10000, l1_ratio=0.4).fit(X_train, y_train)\nlda=da.LinearDiscriminantAnalysis().fit(X_train, y_train)\ncb=catboost.CatBoostClassifier(iterations=500,learning_rate=0.1,depth=6,l2_leaf_reg=3,cat_features=[],eval_metric='Accuracy',verbose=0).fit(X_train, y_train)\nnbayes=nb.GaussianNB().fit(X_train, y_train)\nlda=da.LinearDiscriminantAnalysis(n_components=1,solver='svd').fit(X_train, y_train)\nxg=xgb.XGBClassifier().fit(X_train, y_train)\n\n# Predict\nlgbm_preds = lgbm.predict_proba(X_test).transpose()[1]\nlr_preds = lr.predict_proba(X_test).transpose()[1]\n\nsvm_preds = svmc.predict_proba(X_test).transpose()[1]\nlda_preds = lda.predict_proba(X_test).transpose()[1]\ncb_preds = cb.predict_proba(X_test).transpose()[1]\nnb_preds = nbayes.predict_proba(X_test).transpose()[1]\nlda_preds = lda.predict_proba(X_test).transpose()[1]\nxgb_preds= xg.predict_proba(X_test).transpose()[1]\n\n#Assign different weightages to the classifiers\nensemble_preds = lgbm_preds*0.3 + lr_preds*0.7\nensemble_preds2 = lda_preds*0.5 + cb_preds*0.5\n\nfor x in range(len(ensemble_preds)):\n    if ensemble_preds[x] >= 0.5:\n        ensemble_preds[x] = 1\n    else:\n        ensemble_preds[x] = 0\n\nresults  = ensemble_preds.astype(int)\n\naccuracy =accuracy_score(y_test, results)\nprint(f'Ens1 Accuracy:{accuracy:.4f}')\n\nfor x in range(len(ensemble_preds2)):\n    if ensemble_preds2[x] >= 0.5:\n        ensemble_preds2[x] = 1\n    else:\n        ensemble_preds2[x] = 0\n\nresults  = ensemble_preds2.astype(int)\n\naccuracy =accuracy_score(y_test, results)\nprint(f'Ens2 Accuracy:{accuracy:.4f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T01:04:16.524126Z","iopub.execute_input":"2025-05-14T01:04:16.524446Z","iopub.status.idle":"2025-05-14T01:04:18.276673Z","shell.execute_reply.started":"2025-05-14T01:04:16.524411Z","shell.execute_reply":"2025-05-14T01:04:18.275324Z"}},"outputs":[{"name":"stdout","text":"Ens1 Accuracy:0.8827\nEns2 Accuracy:0.8771\n","output_type":"stream"}],"execution_count":157},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"lgbm_preds = lgbm.predict_proba(X2).transpose()[1]\nlr_preds = lr.predict_proba(X2).transpose()[1]\ncb_preds = cb.predict_proba(X2).transpose()[1]\nlda_preds = lda.predict_proba(X2).transpose()[1]\nensemble_preds = cb_preds*0.5 + lda_preds*0.5\nfor x in range(len(ensemble_preds)):\n    if ensemble_preds[x] >= 0.5:\n        ensemble_preds[x] = 1\n    else:\n        ensemble_preds[x] = 0\n\nresults  = ensemble_preds.astype(int)\n\nsubmission = pd.DataFrame()\nsubmission['Survived'] = results.tolist()\nsubmission['PassengerId']=test_data['PassengerId']\nsubmission.info()\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T00:56:02.223660Z","iopub.execute_input":"2025-05-14T00:56:02.224033Z","iopub.status.idle":"2025-05-14T00:56:02.259652Z","shell.execute_reply.started":"2025-05-14T00:56:02.224002Z","shell.execute_reply":"2025-05-14T00:56:02.258750Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 418 entries, 0 to 417\nData columns (total 2 columns):\n #   Column       Non-Null Count  Dtype\n---  ------       --------------  -----\n 0   Survived     418 non-null    int64\n 1   PassengerId  418 non-null    int64\ndtypes: int64(2)\nmemory usage: 6.7 KB\n","output_type":"stream"}],"execution_count":152},{"cell_type":"markdown","source":"# Ensembled Model","metadata":{}},{"cell_type":"code","source":"model=sklearn.ensemble.StackingClassifier()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"import optuna\nfrom sklearn import linear_model\n\nX_train, X_test,y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=21)\n\ndef objective(trial):\n    # Suggest hyperparameters\n    params = {\n        'penalty':'elasticnet',\n        'dual': False,\n        'solver':'saga',\n        'tol': trial.suggest_int(\"tol\", 0.000001,0.001),\n        'C':trial.suggest_float(\"C\", 0.0, 10.0),\n        'max_iter':trial.suggest_int(\"max_iter\", 10, 500, step=5),\n        'l1_ratio': trial.suggest_float(\"l1_ratio\", 0, 1.0, step=0.1)       \n    }\n\n    model = linear_model.LogisticRegression(**params)\n    scores = cross_val_score(model, X_train, y_train, cv=10, scoring='f1', verbose=0, error_score='raise')\n    \n    return scores.mean()\n    \n# Create a study object\nstudy = optuna.create_study(direction=\"maximize\")\n\n# Optimize the objective function\nstudy.optimize(objective, n_trials=20)\n\nprint(\"Best trial:\", study.best_trial)\nprint(\"Best hyperparameters:\", study.best_params)\nprint(\"Best value:\", study.best_value)\n\nbest_params = study.best_params\nparams = {k: v for k, v in best_params.items()} # Add prefix for set_params function","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, ConfusionMatrixDisplay\n\nbest_model = linear_model.LogisticRegression(**params)\n\n# Initialize and train the best model\nbest_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = best_model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='binary')\nrecall = recall_score(y_test, y_pred, average='binary')\nclass_report = classification_report(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Accuracy: {accuracy:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:39:09.909442Z","iopub.execute_input":"2025-05-11T13:39:09.909826Z","iopub.status.idle":"2025-05-11T13:39:09.958656Z","shell.execute_reply.started":"2025-05-11T13:39:09.909790Z","shell.execute_reply":"2025-05-11T13:39:09.957358Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Processing","metadata":{}},{"cell_type":"code","source":"ignore_features=['Survived','PassengerId']\ncat_col = [col for col, type in train_data.dtypes.items() if type in [\"category\", 'object']and col not in ignore_features]\nnum_col = [col for col, type in train_data.dtypes.items() if type in ['float', 'int64']and col not in ignore_features]\nfeatures = num_col + cat_col\n\nX=train_data.drop(ignore_features, axis=1)\nX_test=test_data.drop('PassengerId', axis=1)\n#train_data['Survived'] = np.where(df['target'] == \"target\", 1, 0)\ny= train_data['Survived'].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:54:29.737739Z","iopub.execute_input":"2025-05-04T14:54:29.738160Z","iopub.status.idle":"2025-05-04T14:54:29.750093Z","shell.execute_reply.started":"2025-05-04T14:54:29.738134Z","shell.execute_reply":"2025-05-04T14:54:29.748468Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Pipeline","metadata":{}},{"cell_type":"code","source":"from typing import List\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn import linear_model\nfrom sklearn.feature_extraction import DictVectorizer\nimport lightgbm as lgb\n\n# Convert df into a dictionary: df_dict\n#df_dict = df.to_dict(\"records\")\n\n# Create the DictVectorizer object: dv\n#dv = DictVectorizer(sparse=False)\n\n# Apply dv on df: df_encoded\n#df_encoded = dv.fit_transform(df_dict)\n\ndef build_pipeline(num_features: List[str], cat_features: List[str]) -> Pipeline:\n    \"\"\"Full pipeline\n\n    This function constructs the whole pipeline for training\n\n    Params:\n        config (Dict): Config content from yaml\n        num_features (List[str]): List of numeric features\n        cat_features (List[str]): List of categorial feature\n\n    Returns:\n        Pipeline that contians pre-process, sampling (If specified) and model\n\n    Note:\n        * Config assumes we're in the `pipeline` level already\n    \"\"\"\n    numeric_transformer = make_pipeline(SimpleImputer(strategy='constant',fill_value=-999),\n                                        StandardScaler()) #strategy='mean'\n\n    categorical_transformer = make_pipeline(SimpleImputer(strategy='most_frequent'), #SimpleImputer(strategy='constant', fill_value='missing'),\n                                            OneHotEncoder(handle_unknown='ignore')) #, min_frequency=0.05\n                                            \n\n    preprocessor = make_column_transformer((numeric_transformer, num_features),(categorical_transformer, cat_features), remainder=\"passthrough\")\n    \n    pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                            ('model', lgb.LGBMClassifier())]) #linear_model.LogisticRegression()\n    return pipe\n\npipeline = build_pipeline(num_features=num_col, cat_features=cat_col)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T14:54:32.979519Z","iopub.execute_input":"2025-05-04T14:54:32.979932Z","iopub.status.idle":"2025-05-04T14:54:32.988815Z","shell.execute_reply.started":"2025-05-04T14:54:32.979902Z","shell.execute_reply":"2025-05-04T14:54:32.987466Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nimport optuna \nimport lightgbm as lgb\n\nX_train, X_test,y_train, y_test=train_test_split(X,y, test_size=0.2, random_state=21)\n\ndef objective(trial):\n    # Suggest hyperparameters\n    params = {\n    #Log Reg\n      #  'model__penalty':'elasticnet',\n      #  'model__dual': False,\n      #  'model__solver':'saga',\n      #  'model__tol': trial.suggest_int(\"tol\", 0.000001,0.001),\n      #  'model__C':trial.suggest_float(\"C\", 0.0, 10.0),\n      #  'model__max_iter':trial.suggest_int(\"max_iter\", 10, 500, step=5),\n      #  'model__l1_ratio': trial.suggest_float(\"l1_ratio\", 0, 1.0, step=0.1)       \n    # LGBM\n        'model__boosting_type':'dart',\n        #'model__categorical_feature': 'name:Sex',\n        #'model__tree_method':'',  # this parameter means using the GPU when training our model to speedup the training process\n        #'model__reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        #'model__reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n        #'model__colsample_bytree': trial.suggest_float('colsample_bytree', 0.3, 1.0),\n        #'model__subsample': trial.suggest_float('subsample', 0.4, 1.0),\n        'model__learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01),         \n        'model__num_iterations': trial.suggest_int('num_iterations', 100, 1000),\n        #'model__max_depth': trial.suggest_int('max_depth',3 , 10),\n        'model__num_leaves':trial.suggest_int('num_leaves',10,100),\n        'model__max_bin': trial.suggest_int('max_bin', 4, 100),\n        'model__n_estimators':trial.suggest_int('n_estimators',100,1000)\n        #'model__min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        #'model__min_child_samples': trial.suggest_int('min_child_samples', 1, 300) # for lgbm    \n    # Catboost\n        \n    \n    }\n    \n\n    model = lgb.LGBMClassifier(**params)\n    scores = cross_val_score(model, X_train, y_train, cv=10, scoring='f1', verbose=0, error_score='raise')\n    \n    return scores.mean()\n    \n# Create a study object\nstudy = optuna.create_study(direction=\"maximize\")\n\n# Optimize the objective function\nstudy.optimize(objective, n_trials=20)\n\nprint(\"Best trial:\", study.best_trial)\nprint(\"Best hyperparameters:\", study.best_params)\nprint(\"Best value:\", study.best_value)\n\nbest_params = study.best_params\nprefix = \"model__\"\nparams = {prefix + k: v for k, v in best_params.items()} # Add prefix for set_params function","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:57:56.687511Z","iopub.execute_input":"2025-05-11T13:57:56.687899Z","iopub.status.idle":"2025-05-11T13:58:14.676252Z","shell.execute_reply.started":"2025-05-11T13:57:56.687871Z","shell.execute_reply":"2025-05-11T13:58:14.675184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Metrics","metadata":{}},{"cell_type":"code","source":"## View feature importance of holdout set model\n## Evaluation holdout set\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, ConfusionMatrixDisplay\n\nbest_model = lgb.LGBMClassifier(**params)\n\n# Initialize and train the best model\nbest_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = best_model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='binary')\nrecall = recall_score(y_test, y_pred, average='binary')\nclass_report = classification_report(y_test, y_pred)\n\n# Print the evaluation metrics\nprint(f'Accuracy: {accuracy:.2f}')\nprint(f'Precision: {precision:.2f}')\nprint(f'Recall: {recall:.2f}')\nprint(f'Classification Report:\\n{class_report}')\n\n# Confusion Matrix\nimport matplotlib.pyplot as plt \nConfusionMatrixDisplay.from_predictions(y_test, y_pred)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Basic Feature Importance Visualisation with Single Decision Tree ","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\np2 = pipeline.named_steps.preprocessor\np2.fit(X_train, y_train)\nx_transf = p2.transform(X_test)\n\n# Initialize the model\nclf = DecisionTreeClassifier(max_depth=2)\n\n# Fit the model to the data\nclf.fit(x_transf, y_test)\n\n# Plot the decision tree\n#plt.figure(figsize=(20, 10))\ntree.plot_tree(clf, filled=True, feature_names=p2.get_feature_names_out(), class_names=['0','1'])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T03:19:41.576874Z","iopub.execute_input":"2025-03-31T03:19:41.577185Z","iopub.status.idle":"2025-03-31T03:19:41.835975Z","shell.execute_reply.started":"2025-03-31T03:19:41.577161Z","shell.execute_reply":"2025-03-31T03:19:41.834894Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Formatting for Submission","metadata":{}},{"cell_type":"code","source":"test_data.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:58:40.866560Z","iopub.execute_input":"2025-05-11T13:58:40.866945Z","iopub.status.idle":"2025-05-11T13:58:40.885655Z","shell.execute_reply.started":"2025-05-11T13:58:40.866917Z","shell.execute_reply":"2025-05-11T13:58:40.884155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Pred_Survived'] = best_model.predict(test_data).tolist()\nsubmission['PassengerId']=test_data['PassengerId']\nsubmission.info()\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T13:58:35.526847Z","iopub.execute_input":"2025-05-11T13:58:35.527218Z","iopub.status.idle":"2025-05-11T13:58:35.556103Z","shell.execute_reply.started":"2025-05-11T13:58:35.527191Z","shell.execute_reply":"2025-05-11T13:58:35.554442Z"}},"outputs":[],"execution_count":null}]}